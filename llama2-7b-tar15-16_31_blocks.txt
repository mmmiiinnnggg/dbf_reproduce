collecting Q input activations for next block ...
collecting FP target output activations for block 16...
16 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03388900552818086
epoch: 1, train err: 0.03134766425137059
epoch: 2, train err: 0.03035971268400317
epoch: 3, train err: 0.029975224439112935
Factorizing 16.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1626381096346661 

16 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 16.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18747302253245732 

16 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 16.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1945561637249037 

16 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03181342336392845
epoch: 1, train err: 0.03013443574673147
epoch: 2, train err: 0.02958312397095142
epoch: 3, train err: 0.029351093522564042
Factorizing 16.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.15895718948884227 

16 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 16.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18195586979273026 

16 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 16.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16848307950838115 

16 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 16.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1884537736840791 

collecting Q input activations for next block ...
collecting FP target output activations for block 17...
17 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03565072662604507
epoch: 1, train err: 0.032860837869520765
epoch: 2, train err: 0.03185805469183833
epoch: 3, train err: 0.03144033416901948
Factorizing 17.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18004851504670796 

17 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 17.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18978534221766677 

17 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 17.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.19974413625584453 

17 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.032874180884391535
epoch: 1, train err: 0.03144353556126589
epoch: 2, train err: 0.03086938244450721
epoch: 3, train err: 0.03061076136509655
Factorizing 17.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17356911090425747 

17 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 17.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1804723710283414 

17 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 17.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16748694404215145 

17 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 17.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18722544202496957 

collecting Q input activations for next block ...
collecting FP target output activations for block 18...
18 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03714962420781376
epoch: 1, train err: 0.03449715977330925
epoch: 2, train err: 0.03347594470687909
epoch: 3, train err: 0.0330554196916637
Factorizing 18.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18800975820871496 

18 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 18.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.19012120924922163 

18 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 18.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.204667395186574 

18 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.033926220930879936
epoch: 1, train err: 0.03264342666079756
epoch: 2, train err: 0.03205935650476022
epoch: 3, train err: 0.03180002432054607
Factorizing 18.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18370370998826793 

18 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 18.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18224541319412257 

18 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 18.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1690669331839128 

18 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 18.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18851081420808058 

collecting Q input activations for next block ...
collecting FP target output activations for block 19...
19 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03836188522836892
epoch: 1, train err: 0.03572207172692288
epoch: 2, train err: 0.03472291986690834
epoch: 3, train err: 0.03433799196500331
Factorizing 19.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.181860974475241 

19 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 19.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18565520688219841 

19 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 19.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.2023838722610011 

19 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03482131122291321
epoch: 1, train err: 0.033620975438680034
epoch: 2, train err: 0.033048034289095085
epoch: 3, train err: 0.032788416334369686
Factorizing 19.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.174298428418267 

19 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 19.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1821725903385337 

19 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 19.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17004782912235755 

19 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 19.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1892726684369748 

collecting Q input activations for next block ...
collecting FP target output activations for block 20...
20 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.039484604298195336
epoch: 1, train err: 0.03698206824628869
epoch: 2, train err: 0.03596799451042898
epoch: 3, train err: 0.03555055856122635
Factorizing 20.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17396128946347633 

20 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 20.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1864523141480142 

20 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 20.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.2120307331286491 

20 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03630926705227466
epoch: 1, train err: 0.03501832826441387
epoch: 2, train err: 0.03440889195189811
epoch: 3, train err: 0.034149984778196085
Factorizing 20.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16641323822062523 

20 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 20.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18050440316580382 

20 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 20.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16930639629860772 

20 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 20.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19249353164805985 

collecting Q input activations for next block ...
collecting FP target output activations for block 21...
21 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.039897967253637034
epoch: 1, train err: 0.03761501726694405
epoch: 2, train err: 0.03668652974010911
epoch: 3, train err: 0.03629895328776911
Factorizing 21.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1723509966308501 

21 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 21.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18425800881834598 

21 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 21.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.2045918017007554 

21 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03730692439421546
epoch: 1, train err: 0.0360178662740509
epoch: 2, train err: 0.03543405988602899
epoch: 3, train err: 0.035195774798921775
Factorizing 21.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16964957603657366 

21 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 21.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17999208442217202 

21 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 21.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16918659301160713 

21 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 21.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19109962987181275 

collecting Q input activations for next block ...
collecting FP target output activations for block 22...
22 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03947798622539267
epoch: 1, train err: 0.037503728533920366
epoch: 2, train err: 0.03666673880070448
epoch: 3, train err: 0.036302356944361236
Factorizing 22.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1759754298897619 

22 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 22.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.19003442446444485 

22 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 22.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.2075651104019609 

22 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.037550535605987534
epoch: 1, train err: 0.03628256858792156
epoch: 2, train err: 0.035730868417886086
epoch: 3, train err: 0.035504963059793226
Factorizing 22.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17593224570418012 

22 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 22.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1793531729715609 

22 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 22.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16906522811934002 

22 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 22.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1900135640661773 

collecting Q input activations for next block ...
collecting FP target output activations for block 23...
23 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03749607159988955
epoch: 1, train err: 0.03585078127798624
epoch: 2, train err: 0.03513190523517551
epoch: 3, train err: 0.03483273162419209
Factorizing 23.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18656897147318352 

23 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 23.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18844946846609362 

23 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 23.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.20381515501696856 

23 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.035484686981362756
epoch: 1, train err: 0.03452696360182017
epoch: 2, train err: 0.03405738330184249
epoch: 3, train err: 0.033866554869746324
Factorizing 23.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18286800336944145 

23 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 23.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18023176341744337 

23 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 23.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17089959034374902 

23 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 23.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18805319215698343 

collecting Q input activations for next block ...
collecting FP target output activations for block 24...
24 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03615834722586442
epoch: 1, train err: 0.034662257981835864
epoch: 2, train err: 0.03400204299396137
epoch: 3, train err: 0.033705008077959064
Factorizing 24.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16672601720651142 

24 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 24.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18591114341230475 

24 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 24.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.21273930189645046 

24 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.034711839820374735
epoch: 1, train err: 0.033772284725273494
epoch: 2, train err: 0.033339013425575104
epoch: 3, train err: 0.033160782608320005
Factorizing 24.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16744098137789185 

24 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 24.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18094100135720778 

24 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 24.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17188610126602427 

24 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 24.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18642154715464412 

collecting Q input activations for next block ...
collecting FP target output activations for block 25...
25 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.034060028512612917
epoch: 1, train err: 0.032798464155348483
epoch: 2, train err: 0.0322021143801976
epoch: 3, train err: 0.031924838716804516
Factorizing 25.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1815334272175839 

25 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 25.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18901805567908922 

25 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 25.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.20455000408156315 

25 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03300450093229301
epoch: 1, train err: 0.032147205725777894
epoch: 2, train err: 0.0317548643797636
epoch: 3, train err: 0.031591714156093076
Factorizing 25.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18372039092734258 

25 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 25.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1826581529145827 

25 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 25.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17358579686608083 

25 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 25.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1858427715231755 

collecting Q input activations for next block ...
collecting FP target output activations for block 26...
26 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.032442938718304504
epoch: 1, train err: 0.03126535561023047
epoch: 2, train err: 0.030718787747900933
epoch: 3, train err: 0.03044678919832222
Factorizing 26.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17707829104512962 

26 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 26.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18876610259183166 

26 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 26.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.22358212349981377 

26 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.0316482133421232
epoch: 1, train err: 0.03082173961593071
epoch: 2, train err: 0.030445989541476592
epoch: 3, train err: 0.03028476711915573
Factorizing 26.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1744684760870531 

26 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 26.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18465328489864785 

26 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 26.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1742235432972921 

26 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 26.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1872351670213008 

collecting Q input activations for next block ...
collecting FP target output activations for block 27...
27 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.030345664454216603
epoch: 1, train err: 0.02927972102770582
epoch: 2, train err: 0.028802549415559042
epoch: 3, train err: 0.02855068611097522
Factorizing 27.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.19329125601908104 

27 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 27.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.19365729468112858 

27 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 27.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.2145124742461135 

27 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.029555663975770585
epoch: 1, train err: 0.02880400508729508
epoch: 2, train err: 0.028460173358325846
epoch: 3, train err: 0.028303441518801264
Factorizing 27.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.19399357903271686 

27 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 27.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1874641996924377 

27 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 27.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17608772401359324 

27 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 27.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18961403335580163 

collecting Q input activations for next block ...
collecting FP target output activations for block 28...
28 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.02925341519585345
epoch: 1, train err: 0.028111959727539215
epoch: 2, train err: 0.027591591839154717
epoch: 3, train err: 0.027305249801429454
Factorizing 28.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.19553498622676146 

28 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 28.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.19538420491302716 

28 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 28.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.22072920613056257 

28 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.02862374694814207
epoch: 1, train err: 0.02777663405140629
epoch: 2, train err: 0.02741352956945775
epoch: 3, train err: 0.02723573212279007
Factorizing 28.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.20448828766403704 

28 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 28.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19159974165927401 

28 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 28.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17715294222749464 

28 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 28.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19659880656870193 

collecting Q input activations for next block ...
collecting FP target output activations for block 29...
29 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.029337929918256123
epoch: 1, train err: 0.02788757257803809
epoch: 2, train err: 0.02729952753725229
epoch: 3, train err: 0.026993048180884216
Factorizing 29.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.177775829564853 

29 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 29.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.19224176188442446 

29 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 29.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.2092161156041306 

29 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.028283901170652825
epoch: 1, train err: 0.02741186472849222
epoch: 2, train err: 0.027014779938326683
epoch: 3, train err: 0.026806060566741508
Factorizing 29.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17917269705972247 

29 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 29.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19642938849454825 

29 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 29.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18086204792249705 

29 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 29.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.20388509338353272 

collecting Q input activations for next block ...
collecting FP target output activations for block 30...
30 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.05023017901839921
epoch: 1, train err: 0.03923106680304045
epoch: 2, train err: 0.035063059731328394
epoch: 3, train err: 0.034093782385753
Factorizing 30.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1869904561315023 

30 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 30.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1978749680719611 

30 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 30.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.22629142072562586 

30 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03714064252562821
epoch: 1, train err: 0.03433196785772452
epoch: 2, train err: 0.03358875500271097
epoch: 3, train err: 0.03320468804304255
Factorizing 30.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18926772461988967 

30 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 30.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.20238480860398858 

30 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 30.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1798771086087355 

30 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 30.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.2168303522215587 

collecting Q input activations for next block ...
collecting FP target output activations for block 31...
31 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.13943132865824737
epoch: 1, train err: 0.11331145391159225
epoch: 2, train err: 0.10737892135512084
epoch: 3, train err: 0.10477957218245137
Factorizing 31.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17555898124345753 

31 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 31.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.19730401528718997 

31 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 31.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.20795611924667326 

31 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.12493406573776156
epoch: 1, train err: 0.11656618375855032
epoch: 2, train err: 0.11304659750021528
epoch: 3, train err: 0.11185086966725066
Factorizing 31.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1761743762516219 

31 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 31.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.21066752974019862 

31 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 31.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18301156030553276 

31 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 31.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.2343591284083519 

total time 62538.561091423035
saving model...
Model's save dict has been saved!
Testing PPL...

Start evaluation...
