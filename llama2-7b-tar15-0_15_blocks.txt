Starting layer-wise PTQ...
collecting activations...
Ready.
collecting FP target output activations for block 0...
0 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 0.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.26095029063658126 

0 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 0.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.2151978421550342 

0 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 0.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1937754189428486 

0 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.00012845520723914206
epoch: 1, train err: 5.396138470104006e-05
epoch: 2, train err: 4.383980664357523e-05
epoch: 3, train err: 2.7639834861759027e-05
Factorizing 0.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1377605031259865 

0 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 0.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16612508442176702 

0 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 0.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.15664791272679718 

0 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 0.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19259044104665363 

collecting Q input activations for next block ...
collecting FP target output activations for block 1...
1 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 2.1510397613674286
epoch: 1, train err: 1.852634757400665
epoch: 2, train err: 0.4682265212904895
epoch: 3, train err: 0.15753540911828168
Factorizing 1.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.10316960577513858 

1 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 1.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.27423349015789367 

1 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 1.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.15530263537650962 

1 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 1.064566696237307
epoch: 1, train err: 0.33900160852863337
epoch: 2, train err: 0.25437926465747296
epoch: 3, train err: 0.2370618601271417
Factorizing 1.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.11026952577415199 

1 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 1.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16240646831292102 

1 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 1.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1370484013293821 

1 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 1.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.20815252732706863 

collecting Q input activations for next block ...
collecting FP target output activations for block 2...
2 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 1.7245774301554775
epoch: 1, train err: 1.5915467031300068
epoch: 2, train err: 1.5128991212404799
epoch: 3, train err: 1.4861909889586968
Factorizing 2.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1293836425000751 

2 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 2.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17872826654193505 

2 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 2.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.21284110486282667 

2 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 1.5444358655659016
epoch: 1, train err: 1.5250774565938627
epoch: 2, train err: 1.516986338843708
epoch: 3, train err: 1.5142977766445256
Factorizing 2.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.11913855345203934 

2 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 2.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17198142819895607 

2 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 2.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.15600527279955087 

2 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 2.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1897224444279444 

collecting Q input activations for next block ...
collecting FP target output activations for block 3...
3 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 1.180386003368767
epoch: 1, train err: 1.1020920401933836
epoch: 2, train err: 1.0528114874614403
epoch: 3, train err: 1.034844825968321
Factorizing 3.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16269358177123763 

3 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 3.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18156427202429498 

3 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 3.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1963899382259535 

3 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 1.0582122571649961
epoch: 1, train err: 1.0397767201066017
epoch: 2, train err: 1.032218532905972
epoch: 3, train err: 1.0297374430519994
Factorizing 3.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1628716736237989 

3 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 3.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1849112867899456 

3 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 3.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1751262433295732 

3 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 3.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18124992337689982 

collecting Q input activations for next block ...
collecting FP target output activations for block 4...
4 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.27126049820071785
epoch: 1, train err: 0.2596511160663795
epoch: 2, train err: 0.254468182232813
epoch: 3, train err: 0.2524338776784134
Factorizing 4.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16084098819585788 

4 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 4.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1816354617585527 

4 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 4.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.19433388677911953 

4 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.2587478291388834
epoch: 1, train err: 0.2531135232493398
epoch: 2, train err: 0.2511517809543875
epoch: 3, train err: 0.2504003099093097
Factorizing 4.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16094486433851565 

4 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 4.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18772735614089195 

4 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 4.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17512797180175718 

4 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 4.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18157695302043478 

collecting Q input activations for next block ...
collecting FP target output activations for block 5...
5 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.1440581679053139
epoch: 1, train err: 0.13639379413143615
epoch: 2, train err: 0.13222703598512453
epoch: 3, train err: 0.1306211238152173
Factorizing 5.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16837833301331967 

5 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 5.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1819093782121537 

5 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 5.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.20024727151093638 

5 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.13525464534177445
epoch: 1, train err: 0.1329107194687822
epoch: 2, train err: 0.13194636039042962
epoch: 3, train err: 0.1315788088431873
Factorizing 5.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1657762729274271 

5 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 5.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18664934362018007 

5 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 5.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17269343461562076 

5 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 5.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17780956698954584 

collecting Q input activations for next block ...
collecting FP target output activations for block 6...
6 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.08614850676895003
epoch: 1, train err: 0.0816406377707608
epoch: 2, train err: 0.07966235506683006
epoch: 3, train err: 0.07886578798570554
Factorizing 6.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.15515458377018007 

6 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 6.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17705089426484408 

6 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 6.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1810990126562231 

6 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.08199205216806149
epoch: 1, train err: 0.0803722254022432
epoch: 2, train err: 0.07975585954045528
epoch: 3, train err: 0.07949928078232915
Factorizing 6.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1598672560194055 

6 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 6.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18707888899263964 

6 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 6.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17127875563984846 

6 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 6.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17737474937915343 

collecting Q input activations for next block ...
collecting FP target output activations for block 7...
7 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.06541389060294023
epoch: 1, train err: 0.062079161194560584
epoch: 2, train err: 0.06057265753406682
epoch: 3, train err: 0.059929121442110045
Factorizing 7.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1540017912445726 

7 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 7.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1778275575937332 

7 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 7.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1929543308411395 

7 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.0621601322418428
epoch: 1, train err: 0.060879867131006904
epoch: 2, train err: 0.060357299607858295
epoch: 3, train err: 0.06013311462083948
Factorizing 7.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.15625939036165362 

7 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 7.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1854642140194656 

7 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 7.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1689049923420341 

7 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 7.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17504373190824818 

collecting Q input activations for next block ...
collecting FP target output activations for block 8...
8 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.05074259117463953
epoch: 1, train err: 0.0484418614905735
epoch: 2, train err: 0.04752702103724005
epoch: 3, train err: 0.04712293373995635
Factorizing 8.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16024506352692514 

8 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 8.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17433135124553417 

8 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 8.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.19461080159818875 

8 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.04865566761509399
epoch: 1, train err: 0.04761745545511076
epoch: 2, train err: 0.04720513527536241
epoch: 3, train err: 0.04701767804363044
Factorizing 8.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16631008009920084 

8 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 8.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18425620960815817 

8 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 8.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1684628825438225 

8 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 8.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17353929618052627 

collecting Q input activations for next block ...
collecting FP target output activations for block 9...
9 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.04521628213842632
epoch: 1, train err: 0.04296772168527241
epoch: 2, train err: 0.04220988378983748
epoch: 3, train err: 0.04186519492395746
Factorizing 9.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16360118283257585 

9 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 9.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1751948372371142 

9 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 9.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1968031307431289 

9 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.043425204705272336
epoch: 1, train err: 0.04235754587534757
epoch: 2, train err: 0.041945459288399434
epoch: 3, train err: 0.0417535092565231
Factorizing 9.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16272695584380514 

9 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 9.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18353988428672935 

9 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 9.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16851671626992173 

9 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 9.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17330431564175286 

collecting Q input activations for next block ...
collecting FP target output activations for block 10...
10 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03978179310433916
epoch: 1, train err: 0.03787936045409879
epoch: 2, train err: 0.037113556205440545
epoch: 3, train err: 0.036785735179364565
Factorizing 10.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.15256941113207734 

10 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 10.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17012427201108965 

10 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 10.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17752293416717566 

10 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03819315546206781
epoch: 1, train err: 0.037133132747840136
epoch: 2, train err: 0.03677074806910241
epoch: 3, train err: 0.03657571143048699
Factorizing 10.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.15217508838553376 

10 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 10.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18287116682973545 

10 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 10.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16837126909614125 

10 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 10.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1716974132303522 

collecting Q input activations for next block ...
collecting FP target output activations for block 11...
11 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.035078531411272706
epoch: 1, train err: 0.0330990955735615
epoch: 2, train err: 0.03234185491965036
epoch: 3, train err: 0.03201160046228324
Factorizing 11.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1434451005862372 

11 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 11.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16783425023925003 

11 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 11.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1716399877851311 

11 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03321043655159883
epoch: 1, train err: 0.03221498764469288
epoch: 2, train err: 0.03185123645380372
epoch: 3, train err: 0.03165298454769072
Factorizing 11.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1504211284173683 

11 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 11.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1814972377507399 

11 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 11.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1667413559790859 

11 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 11.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17364591202749946 

collecting Q input activations for next block ...
collecting FP target output activations for block 12...
12 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.034290669529582374
epoch: 1, train err: 0.03194036501736264
epoch: 2, train err: 0.031224957107042428
epoch: 3, train err: 0.03087090770713985
Factorizing 12.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16433207508229378 

12 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 12.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17589520069354833 

12 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 12.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18559452009508473 

12 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03227719200003776
epoch: 1, train err: 0.031232575103786075
epoch: 2, train err: 0.030853704392939107
epoch: 3, train err: 0.030628081618488068
Factorizing 12.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.15868332976876698 

12 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 12.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18097640498959383 

12 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 12.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16744031211258523 

12 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 12.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1742865242802721 

collecting Q input activations for next block ...
collecting FP target output activations for block 13...
13 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.032742833460361
epoch: 1, train err: 0.030373119308933383
epoch: 2, train err: 0.02957163442624733
epoch: 3, train err: 0.029169700646889396
Factorizing 13.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16518767867195555 

13 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 13.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18530353378194792 

13 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 13.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18185671003755782 

13 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.030821023356111255
epoch: 1, train err: 0.02960238154992112
epoch: 2, train err: 0.02913319343861076
epoch: 3, train err: 0.02887555158304167
Factorizing 13.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16768184448890908 

13 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 13.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1830524420422397 

13 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 13.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16967253999314985 

13 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 13.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17831648922004098 

collecting Q input activations for next block ...
collecting FP target output activations for block 14...
14 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.0307069994014455
epoch: 1, train err: 0.028518523442471633
epoch: 2, train err: 0.027762602399889147
epoch: 3, train err: 0.027440789985121228
Factorizing 14.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1545734530397874 

14 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 14.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17943785035626855 

14 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 14.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1801038664675124 

14 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.028653388162638294
epoch: 1, train err: 0.02760107493668329
epoch: 2, train err: 0.027177970547199948
epoch: 3, train err: 0.026967048321239417
Factorizing 14.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.15406862712860386 

14 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 14.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18134759291456148 

14 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 14.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16860631478554708 

14 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 14.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17993800670284177 

collecting Q input activations for next block ...
collecting FP target output activations for block 15...
15 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.03302502683800412
epoch: 1, train err: 0.03056728067167569
epoch: 2, train err: 0.029690462484722957
epoch: 3, train err: 0.02931932491628686
Factorizing 15.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16505809204284771 

15 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 15.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18109355289994453 

15 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 15.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18258941644339713 

15 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.030381447490071878
epoch: 1, train err: 0.02926866544657969
epoch: 2, train err: 0.028796697548386874
epoch: 3, train err: 0.028593074399395846
Factorizing 15.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16148308618686016 

15 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 15.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1844565293086488 

15 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 15.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17101130907006762 

15 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 15.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18532641556930637 
