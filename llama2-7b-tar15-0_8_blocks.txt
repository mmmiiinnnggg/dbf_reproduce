Starting layer-wise PTQ...
collecting activations...
Ready.
collecting FP target output activations for block 0...
0 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 0.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.26095029063658126 

0 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 0.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.2151978421550342 

0 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 0.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1937754189428486 

0 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.00012845520723914206
epoch: 1, train err: 5.396138470104006e-05
epoch: 2, train err: 4.383980664357523e-05
epoch: 3, train err: 2.7639834861759027e-05
Factorizing 0.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1377605031259865 

0 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 0.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16612508442176702 

0 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 0.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.15664791272679718 

0 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 0.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.19259044104665363 

collecting Q input activations for next block ...
collecting FP target output activations for block 1...
1 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 2.1510397613674286
epoch: 1, train err: 1.852634757400665
epoch: 2, train err: 0.4682265212904895
epoch: 3, train err: 0.15753540911828168
Factorizing 1.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.10316960577513858 

1 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 1.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.27423349015789367 

1 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 1.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.15530263537650962 

1 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 1.064566696237307
epoch: 1, train err: 0.33900160852863337
epoch: 2, train err: 0.25437926465747296
epoch: 3, train err: 0.2370618601271417
Factorizing 1.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.11026952577415199 

1 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 1.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.16240646831292102 

1 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 1.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1370484013293821 

1 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 1.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.20815252732706863 

collecting Q input activations for next block ...
collecting FP target output activations for block 2...
2 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 1.7245774301554775
epoch: 1, train err: 1.5915467031300068
epoch: 2, train err: 1.5128991212404799
epoch: 3, train err: 1.4861909889586968
Factorizing 2.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1293836425000751 

2 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 2.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17872826654193505 

2 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 2.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.21284110486282667 

2 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 1.5444358655659016
epoch: 1, train err: 1.5250774565938627
epoch: 2, train err: 1.516986338843708
epoch: 3, train err: 1.5142977766445256
Factorizing 2.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.11913855345203934 

2 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 2.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17198142819895607 

2 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 2.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.15600527279955087 

2 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 2.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1897224444279444 

collecting Q input activations for next block ...
collecting FP target output activations for block 3...
3 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 1.180386003368767
epoch: 1, train err: 1.1020920401933836
epoch: 2, train err: 1.0528114874614403
epoch: 3, train err: 1.034844825968321
Factorizing 3.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16269358177123763 

3 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 3.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.18156427202429498 

3 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 3.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1963899382259535 

3 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 1.0582122571649961
epoch: 1, train err: 1.0397767201066017
epoch: 2, train err: 1.032218532905972
epoch: 3, train err: 1.0297374430519994
Factorizing 3.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1628716736237989 

3 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 3.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1849112867899456 

3 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 3.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1751262433295732 

3 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 3.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18124992337689982 

collecting Q input activations for next block ...
collecting FP target output activations for block 4...
4 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.27126049820071785
epoch: 1, train err: 0.2596511160663795
epoch: 2, train err: 0.254468182232813
epoch: 3, train err: 0.2524338776784134
Factorizing 4.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16084098819585788 

4 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 4.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1816354617585527 

4 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 4.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.19433388677911953 

4 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.2587478291388834
epoch: 1, train err: 0.2531135232493398
epoch: 2, train err: 0.2511517809543875
epoch: 3, train err: 0.2504003099093097
Factorizing 4.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16094486433851565 

4 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 4.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18772735614089195 

4 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 4.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17512797180175718 

4 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 4.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18157695302043478 

collecting Q input activations for next block ...
collecting FP target output activations for block 5...
5 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.1440581679053139
epoch: 1, train err: 0.13639379413143615
epoch: 2, train err: 0.13222703598512453
epoch: 3, train err: 0.1306211238152173
Factorizing 5.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.16837833301331967 

5 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 5.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1819093782121537 

5 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 5.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.20024727151093638 

5 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.13525464534177445
epoch: 1, train err: 0.1329107194687822
epoch: 2, train err: 0.13194636039042962
epoch: 3, train err: 0.1315788088431873
Factorizing 5.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1657762729274271 

5 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 5.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18664934362018007 

5 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 5.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17269343461562076 

5 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 5.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17780956698954584 

collecting Q input activations for next block ...
collecting FP target output activations for block 6...
6 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.08614850676895003
epoch: 1, train err: 0.0816406377707608
epoch: 2, train err: 0.07966235506683006
epoch: 3, train err: 0.07886578798570554
Factorizing 6.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.15515458377018007 

6 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 6.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.17705089426484408 

6 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 6.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1810990126562231 

6 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.08199205216806149
epoch: 1, train err: 0.0803722254022432
epoch: 2, train err: 0.07975585954045528
epoch: 3, train err: 0.07949928078232915
Factorizing 6.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1598672560194055 

6 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 6.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.18707888899263964 

6 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 6.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17127875563984846 

6 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 6.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17737474937915343 

collecting Q input activations for next block ...
collecting FP target output activations for block 7...
7 self_attn.q_proj
self_attn.q_proj.weight torch.float32
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.q_proj.weight', 'self_attn.k_proj.weight', 'self_attn.v_proj.weight', 'self_attn.o_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.06541389060294023
epoch: 1, train err: 0.062079161194560584
epoch: 2, train err: 0.06057265753406682
epoch: 3, train err: 0.059929121442110045
Factorizing 7.self_attn.q_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1540017912445726 

7 self_attn.v_proj
self_attn.k_proj.weight torch.float32
self_attn.v_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 7.self_attn.v_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1778275575937332 

7 self_attn.o_proj
self_attn.k_proj.weight torch.float32
self_attn.o_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 7.self_attn.o_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.1929543308411395 

7 self_attn.k_proj
self_attn.k_proj.weight torch.float32
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
tuning weights:  dict_keys(['self_attn.k_proj.weight', 'mlp.gate_proj.weight', 'mlp.up_proj.weight', 'mlp.down_proj.weight'])
epoch: 0, train err: 0.0621601322418428
epoch: 1, train err: 0.060879867131006904
epoch: 2, train err: 0.060357299607858295
epoch: 3, train err: 0.06013311462083948
Factorizing 7.self_attn.k_proj...
sparsity check:  1.5 bpw
relative err after factorization:  0.15625939036165362 

7 mlp.up_proj
mlp.gate_proj.weight torch.float32
mlp.up_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 7.mlp.up_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1854642140194656 

7 mlp.gate_proj
mlp.gate_proj.weight torch.float32
mlp.down_proj.weight torch.float32
Factorizing 7.mlp.gate_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.1689049923420341 

7 mlp.down_proj
mlp.down_proj.weight torch.float32
Factorizing 7.mlp.down_proj...
sparsity check:  1.499721793241279 bpw
relative err after factorization:  0.17504373190824818 
